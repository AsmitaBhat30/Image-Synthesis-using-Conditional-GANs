{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Activation, Dense, Input\n",
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers import Reshape, Conv2DTranspose\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.utils import to_categorical, plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from numpy.random import randint\n",
    "\n",
    "\n",
    "class CGAN:\n",
    "    def __init__(self, img_width, img_height, n_channels, n_classes):\n",
    "        self.img_width = img_width\n",
    "        self.img_height = img_height\n",
    "        self.n_channels = n_channels\n",
    "        self.img_shape = (self.img_width, self.img_height, self.n_channels)\n",
    "        self.n_classes = n_classes\n",
    "        self.latent_dim = 100\n",
    "\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator_model()\n",
    "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
    "                                   optimizer=optimizer,\n",
    "                                   metrics=['accuracy'])\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        label = Input(shape=(self.n_classes,))\n",
    "        img = self.generator([noise, label])\n",
    "\n",
    "        # during generator updating,  the discriminator is fixed (will not be updated).\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The discriminator takes generated image and label as input and determines its validity\n",
    "        validity = self.discriminator([img, label])\n",
    "\n",
    "        self.cgan_model = Model(input=[noise, label], output=validity)\n",
    "        self.cgan_model.compile(loss=['binary_crossentropy'],\n",
    "                                optimizer=optimizer,\n",
    "                                metrics=['accuracy'])\n",
    "\n",
    "        plot_model(self.cgan_model, show_shapes=True, to_file='cgan-adversarial_model.png')\n",
    "        plot_model(self.generator, show_shapes=True, to_file='cgan-generator_model.png')\n",
    "        plot_model(self.discriminator, show_shapes=True, to_file='cgan-discriminator.png')\n",
    "\n",
    "    def build_discriminator_model(self):\n",
    "\n",
    "        model_input = Input(shape=(self.img_width, self.img_height, self.n_channels), name='discriminator_input')\n",
    "\n",
    "        x = model_input\n",
    "\n",
    "        labels = Input(shape=(self.n_classes,))\n",
    "        # labels_embedded = Flatten()(Embedding(self.num_classes, self.latent_dim)(labels))\n",
    "        labels_embedded = Dense(self.img_width * self.img_width)(labels)\n",
    "        labels_embedded = Reshape((self.img_width, self.img_height, self.n_channels))(labels_embedded)\n",
    "\n",
    "        x = concatenate([x, labels_embedded])\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x = Conv2D(filters=32, kernel_size=5, strides=2, padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x = Conv2D(filters=64, kernel_size=5, strides=2, padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x = Conv2D(filters=128, kernel_size=5, strides=2, padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x = Conv2D(filters=256, kernel_size=5, strides=1, padding='same')(x)\n",
    "        x = LeakyReLU(alpha=0.1)(x)\n",
    "\n",
    "        x = Flatten()(x)\n",
    "        x = Dense(1)(x)\n",
    "        x = Activation('sigmoid')(x)\n",
    "        # model_input is conditioned by labels\n",
    "        discriminator = Model([model_input, labels], x, name='discriminator')\n",
    "\n",
    "        return discriminator\n",
    "\n",
    "    def build_generator(self):\n",
    "        image_resize = self.img_height // 4\n",
    "\n",
    "        inputs = Input(shape=(self.latent_dim,), name='z_input')\n",
    "        labels = Input(shape=(self.n_classes,), name='class_labels')\n",
    "\n",
    "        x = concatenate([inputs, labels], axis=1)\n",
    "        x = Dense(image_resize * image_resize * 128)(x)\n",
    "        x = Reshape((image_resize, image_resize, 128))(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=128, kernel_size=5, strides=2, padding='same')(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=64, kernel_size=5, strides=2, padding='same')(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=32, kernel_size=5, strides=1, padding='same')(x)\n",
    "\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Activation('relu')(x)\n",
    "        x = Conv2DTranspose(filters=1, kernel_size=5, strides=1, padding='same')(x)\n",
    "\n",
    "        x = Activation('sigmoid')(x)\n",
    "        # input is conditioned by labels\n",
    "        generator = Model(input=[inputs, labels], output=x, name='generator')\n",
    "        return generator\n",
    "\n",
    "    def train(self, x_train, y_train, epochs=100, batch_size=128, sample_interval=50):\n",
    "\n",
    "        x_train = np.reshape(x_train, [-1, self.img_width, self.img_height, self.n_channels])\n",
    "        x_train = x_train.astype('float32') / 255\n",
    "\n",
    "        y_train = to_categorical(y_train)\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        real = np.ones((batch_size, 1))\n",
    "        fake = np.zeros((batch_size, 1))\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "\n",
    "            #  --------------------- Train Discriminator ---------------------\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, x_train.shape[0], size=batch_size)\n",
    "            imgs, labels = x_train[idx], y_train[idx]\n",
    "\n",
    "            # Generate sample noise for generator input\n",
    "            noise = self.generate_noise(\"uniform_noise\", batch_size)\n",
    "\n",
    "            # Generate a half batch of new images\n",
    "            # we can use labels instead of fake_labels; because it is fake for noise\n",
    "            gen_imgs = self.generator.predict([noise, labels])\n",
    "\n",
    "            # --------------------- Train the Discriminator ---------------------\n",
    "            d_loss_real = self.discriminator.train_on_batch([imgs, labels], real)\n",
    "            d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            #  --------------------- Train the Generator ---------------------\n",
    "            # Condition on labels (random one-hot labels)\n",
    "            fake_labels = np.eye(self.n_classes)[np.random.choice(self.n_classes, batch_size)]\n",
    "\n",
    "            # Train the generator\n",
    "            cgan_loss, acc = self.cgan_model.train_on_batch([noise, fake_labels], real)\n",
    "\n",
    "            # Plot the progress\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100 * d_loss[1], cgan_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "        self.generator.save(\"generator_model_cgan_v2.h5\")\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 2, 5\n",
    "        noise = np.random.uniform(-1.0, 1.0, size=[r * c, self.latent_dim])\n",
    "\n",
    "        sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
    "        sampled_labels_categorical = to_categorical(sampled_labels)\n",
    "\n",
    "        gen_imgs = self.generator.predict([noise, sampled_labels_categorical])\n",
    "        print(gen_imgs)\n",
    "        # Rescale images 0 - 1\n",
    "        # gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, 0], cmap='gray')\n",
    "                axs[i, j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/iter_{}.png\".format(epoch), bbox_inches='tight', dpi=200)\n",
    "        plt.close()\n",
    "\n",
    "    def generate_noise(self, type_of_noise, batch_size):\n",
    "        if type_of_noise == \"normal_noise\":\n",
    "            return np.random.normal(0, 1, size=[batch_size, self.latent_dim])\n",
    "\n",
    "        elif type_of_noise == \"uniform_noise\":\n",
    "            return np.random.uniform(-1.0, 1.0, size=[batch_size, self.latent_dim])\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load the dataset\n",
    "    (X, y), (_, _) = mnist.load_data()\n",
    "    num_classes = 10\n",
    "\n",
    "    if X[0].ndim == 3:\n",
    "        img_w, img_h, num_channels = X[0].shape\n",
    "    else:\n",
    "        img_w, img_h = X[0].shape\n",
    "        num_channels = 1\n",
    "\n",
    "    cgan = CGAN(img_w, img_h, num_channels, num_classes)\n",
    "\n",
    "    cgan.train(X, y, epochs=20000, batch_size=32, sample_interval=300)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
